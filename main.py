from preprocessing import *
from attention_module import *

import torch
import torch.nn as nn
from torch import optim 
import nltk.translate.bleu_score as bleu
import os
import time

def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, MAX_LENGTH, 
            device):
    ############################################
    #   train (Encoder, Decoderì— ë§ê²Œ êµ¬í˜„í•´ì•¼ í•¨)
    #   
    #   <parameters>
    #   - input_tensor  
    #   - target_tensor  
    #   - encoder                   : Encoder ëª¨ë“ˆ
    #   - decoder                   : Decoder ëª¨ë“ˆ
    #   - encoder_optimizer         : Encoder Optim (SGD)
    #   - decoder_optimizer         : Decoder Optim (SGD)  
    #   - criterion                 : Loss ê³„ì‚° (NLLLoss) 
    #   - MAX_LENGTH                : ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´
    #   ğŸ¦
    #   - device                    : GPUë¥¼ ì‚¬ìš©í•  ê²ƒì¸ì§€, CPUë¥¼ ì‚¬ìš©í•  ê²ƒì¸ì§€ ì„ íƒ 
    #
    #    <return>
    #   - encoder                   : Encoder ëª¨ë“ˆ (ì·¨ì†Œ)
    #   - decoder                   : Decoder ëª¨ë“ˆ (ì·¨ì†Œ)
    #   - loss                      : loss
    ############################################

    SOS_token = 2
    EOS_token = 3

    #####################################
    # encoderì™€ decoderì˜ ì´ˆê¸° hidden, cell ì´ˆê¸°í™”
    # decoderëŠ” ì´ˆê¸° hiddenì„ encoderì—ê²Œì„œ ë°›ìœ¼ë¯€ë¡œ,
    # ì´ˆê¸°í™”í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.
    # ì „ë¶€ 3ì°¨ì› ë²¡í„°(?)ì„ì„ ì£¼ì˜.
    #####################################
    encoder_hidden = encoder.initHidden(device)
    encoder_cell = encoder.initCell(device)

    decoder_hidden = None # useless
    decoder_cell = decoder.initCell(device)

    #####################################
    # ëˆ„ì ëœ gradient ì´ˆê¸°í™”
    #####################################
    encoder_optimizer.zero_grad()
    decoder_optimizer.zero_grad()

    #####################################
    # ì…ë ¥ ë¬¸ì¥ì˜ ê¸¸ì´ input_length
    # íƒ€ê²Ÿ ë¬¸ì¥ì˜ ê¸¸ì´ target_length
    #####################################
    input_length = input_tensor.size(0)
    target_length = target_tensor.size(0)

    #####################################
    # ğŸª Pytorch.orgì˜ ê³µì‹ ë ˆí¼ëŸ°ìŠ¤ì— ë”°ë¥´ë©´,
    # encoder_outputsë¥¼ ì´ìš©í•´ì„œ Decoderì—ê²Œ ì „ë‹¬
    # í•˜ì§€ë§Œ, hidden vectorë“¤ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì •ì„ì´ë¯€ë¡œ ìˆ˜ì •í•©ë‹ˆë‹¤.
    #####################################
    # encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)
    hs_encoder_hiddens = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)

    #####################################
    # ì¶œë ¥ìš© loss ì´ˆê¸°í™”
    #####################################
    loss = 0

    #####################################
    # ğŸŒ
    # for-loopë¥¼ í†µí•´ Encoder Forward Propagation
    #####################################
    for ei in range(input_length):
        encoder_output, encoder_hidden, encoder_cell = encoder(
            input_tensor[ei], encoder_hidden, encoder_cell)

        #####################################
        # encoder_hiddenì€ (1 * 1 * h)ì˜ 3ì°¨ì› êµ¬ì¡°ì˜ ë²¡í„°ì´ë‹¤.
        # ë”°ë¼ì„œ, [[[~~~~~]]] ì•ˆì˜ ~~~~~ë¥¼ ì–»ì–´ë‚´ê¸° ìœ„í•´ì„œ
        # encoder_hidden[0, 0]ì„ í†µí•´ ëŒì–´ë‚¸ë‹¤(?)
        #####################################
        hs_encoder_hiddens[ei] = encoder_hidden[0, 0]
    
    decoder_input = torch.tensor([SOS_token], device=device)
    
    #####################################
    # ë§ˆì§€ë§‰ encoderì˜ hidden vectorê°€
    # decoderì˜ ì´ˆê¸° ì…ë ¥ hidden vector
    #####################################
    decoder_hidden = encoder_hidden 

    for di in range(target_length):
        decoder_output, decoder_hidden, decoder_cell, attn_distr = decoder(
            decoder_input, decoder_hidden, decoder_cell, hs_encoder_hiddens)

        loss += criterion(decoder_output, target_tensor[di])
        decoder_input = target_tensor[di]  # Teacher forcing

    loss.backward()

    encoder_optimizer.step()
    decoder_optimizer.step()

    return loss.item() / target_length

def print_index_text(index_text, ind2word, country):

    if country == 0: # ì†ŒìŠ¤ ì–¸ì–´
        print("English Text----")
    else: # íƒ€ê¹ƒì–¸ì–´
        print("French Text----")

    for word in index_text:
        print(ind2word[word], end=' ')
    print('')

def convert_index_text(index_text, ind2word):
    result = []
    for word in index_text:
        result.append(ind2word[word])
    result.pop()
    # print(result)
    return result

def evaluate_Iter(input_tensor, target_tensor, encoder, decoder, source_word2index, target_word2index, MAX_LENGTH, device):
    
    SOS_token = 2
    EOS_token = 3

    #########################################
    # hidden vector ë° cellì˜ í¬ê¸°ëŠ” 256ìœ¼ë¡œ ì„ íƒ
    #########################################
    hidden_size = 256

    #####################################
    # encoderì™€ decoderì˜ ì´ˆê¸° hidden, cell ì´ˆê¸°í™”
    # decoderëŠ” ì´ˆê¸° hiddenì„ encoderì—ê²Œì„œ ë°›ìœ¼ë¯€ë¡œ,
    # ì´ˆê¸°í™”í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.
    # ì „ë¶€ 3ì°¨ì› ë²¡í„°(?)ì„ì„ ì£¼ì˜.
    #####################################
    encoder_hidden = encoder.initHidden(device)
    encoder_cell = encoder.initCell(device)

    decoder_hidden = None # useless
    decoder_cell = decoder.initCell(device)

    #####################################
    # ì…ë ¥ ë¬¸ì¥ì˜ ê¸¸ì´ input_length
    # íƒ€ê²Ÿ ë¬¸ì¥ì˜ ê¸¸ì´ target_length
    #####################################
    input_length = input_tensor.size(0)
    target_length = target_tensor.size(0)

    hs_encoder_hiddens = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)

    #####################################
    # ğŸŒ
    # for-loopë¥¼ í†µí•´ Encoder Forward Propagation
    #####################################
    for ei in range(input_length):
        encoder_output, encoder_hidden, encoder_cell = encoder(
            input_tensor[ei], encoder_hidden, encoder_cell)

        #####################################
        # encoder_hiddenì€ (1 * 1 * h)ì˜ 3ì°¨ì› êµ¬ì¡°ì˜ ë²¡í„°ì´ë‹¤.
        # ë”°ë¼ì„œ, [[[~~~~~]]] ì•ˆì˜ ~~~~~ë¥¼ ì–»ì–´ë‚´ê¸° ìœ„í•´ì„œ
        # encoder_hidden[0, 0]ì„ í†µí•´ ëŒì–´ë‚¸ë‹¤(?)
        #####################################
        hs_encoder_hiddens[ei] = encoder_hidden[0, 0]
    
    decoder_input = torch.tensor([SOS_token], device=device)
    decoded_text = []
    
    #####################################
    # ë§ˆì§€ë§‰ encoderì˜ hidden vectorê°€
    # decoderì˜ ì´ˆê¸° ì…ë ¥ hidden vector
    #####################################
    decoder_hidden = encoder_hidden 

    for di in range(MAX_LENGTH):
        decoder_output, decoder_hidden, decoder_cell, attn_distr = decoder(
            decoder_input, decoder_hidden, decoder_cell, hs_encoder_hiddens)
        
        output_argmax = torch.argmax(decoder_output, dim=1)
        if output_argmax == 3: # EOS
            decoded_text.append(output_argmax)
            break

        decoded_text.append(output_argmax)

        decoder_input = output_argmax

    return torch.tensor(decoded_text, device=device)

def evaluate(encoder, decoder, source_index2word, source_word2index, target_index2word, target_word2index, MAX_LENGTH, device):
    #########################################
    # tokenizing and preprocessing of test data
    #########################################
    print("tokenizing...")
    source_texts, target_texts, target_labels = tokenize('data/eng-fra_test.txt')

    source_ind_texts = wordtext2indtext(source_texts, source_word2index)
    target_ind_texts = wordtext2indtext(target_texts, target_word2index)
    target_ind_labels = wordtext2indtext(target_labels, target_word2index)

    testing_pairs = [(torch.tensor(s, dtype=torch.long, device=device).view(-1, 1), \
            torch.tensor(t, dtype=torch.long, device=device).view(-1, 1)) \
            for s, t in zip(source_ind_texts, target_ind_labels)]

    #########################################
    # Start testing
    #########################################
    print("\n<START TESTING-------------->")   

    total_bleu = 0

    for iter in range(1, len(testing_pairs) + 1):
        testing_pair  = testing_pairs[iter - 1]
        input_tensor = testing_pair[0]
        target_tensor = testing_pair[1]

        decoded_text = evaluate_Iter(input_tensor, target_tensor, encoder, decoder, source_word2index, target_word2index, MAX_LENGTH, device)

        # print('iter %d ------------------------------' % (iter))
        # print_index_text(input_tensor.squeeze().cpu().tolist(), source_index2word, 0)
        # print_index_text(decoded_text.view(1, -1).cpu().tolist()[0], target_index2word, 1)
        
        bleu_references = (convert_index_text(target_tensor.squeeze().cpu().tolist(), target_index2word))
        bleu_hypotheses = (convert_index_text(decoded_text.view(1, -1).cpu().tolist()[0], target_index2word))

        #########################################
        # BLEU ì¸¡ì • (nltk ì„¤ì¹˜ í•„ìˆ˜)
        #########################################
        total_bleu += bleu.sentence_bleu([bleu_references], bleu_hypotheses)

    print('average BLEU score : %.3f\n' % (total_bleu / len(testing_pairs)))

    return (total_bleu / len(testing_pairs))

def main():
    
    #########################################
    # cudaë¥¼ ì‚¬ìš©í•  ê²ƒì¸ì§€, cpuë¥¼ ì‚¬ìš©í•  ê²ƒì¸ì§€ ì„ íƒ
    #########################################
    print("device setting...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print("we will use...."+device+"!\n")
    device = torch.device(device)

    #########################################
    # tokenizing and preprocessing of train data
    #########################################
    print("tokenizing...")
    source_texts, target_texts, target_labels = tokenize('data/eng-fra_train.txt')

    source_word2index, source_index2word = preprocess(source_texts)
    target_word2index, target_index2word = preprocess(target_texts)

    source_ind_texts = wordtext2indtext(source_texts, source_word2index)
    target_ind_texts = wordtext2indtext(target_texts, target_word2index)
    target_ind_labels = wordtext2indtext(target_labels, target_word2index)

    source_max_length = max([len(each) for each in source_ind_texts])
    target_max_length = max([len(each) for each in target_ind_texts])
    MAX_LENGTH = max(source_max_length, target_max_length)

    print("------------------------------------")
    print("SOURCE WORD2INDEX MAX LENGTH : ", source_max_length)
    print("TARGET WORD2INDEX MAX LENGTH : ", target_max_length)

    #########################################
    # í•™ìŠµì— ì‚¬ìš©í•  indexí™” ì‹œí‚¨ ì˜ì–´ ë¬¸ì¥ê³¼
    # í”„ë¼ìŠ¤ì–´ ë¬¸ì¥ì„ pairë¡œ ë¬¶ì–´ì„œ training_pairsì— ì €ì¥
    #
    # ğŸ‘ ë¯¸ë¦¬ target_ind_labelsìœ¼ë¡œ pairsì— ì €ì¥í•˜ì‹  ê²ƒ íƒì›”í–ˆìŠµë‹ˆë‹¤.
    #
    #########################################
    training_pairs = [(torch.tensor(s, dtype=torch.long, device=device).view(-1, 1), \
              torch.tensor(t, dtype=torch.long, device=device).view(-1, 1)) \
             for s, t in zip(source_ind_texts, target_ind_labels)]
    
    #########################################
    # hidden vector ë° cellì˜ í¬ê¸°ëŠ” 256ìœ¼ë¡œ ì„ íƒ
    #########################################
    hidden_size = 256

    #########################################
    # encoderì™€ decoder ê°ì²´ ìƒì„±
    #########################################
    encoder = Encoder(len(source_word2index), hidden_size).to(device)
    decoder = Decoder(len(target_word2index), hidden_size).to(device)
    print("finished making Encoder and Decoder...")

    #########################################
    # encoderì™€ decoder ê°ê°ì— ëŒ€í•œ optimizer ìƒì„±
    # ìˆœìˆ˜í•œ SGD ë°©ì‹ ì„ íƒ
    #########################################
    learning_rate = 0.01
    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)
    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)
    print("finished making Encoder_optimizer and Decoder_optimizer...")

    #########################################
    # Loss functionì€ Negative Log Likelihoodë¥¼ ì‚¬ìš©
    #########################################
    criterion = nn.NLLLoss()
    
    #########################################
    # í•™ìŠµë¥  ì¸¡ì •ì— í•„ìš”í•œ ë³€ìˆ˜ë“¤
    # - ckpt : ë§¤ë²ˆ 1000ë²ˆ iterationë§ˆë‹¤ í‰ê·  lossë¥¼ ì¶œë ¥
    #########################################
    loss_total = 0
    iter_total = 0
    epoches = 50
    
    #########################################
    # Start training
    #########################################
    print("\n<START TRAINING-------------->")    
    encoder.train()
    decoder.train()

    start = time.time()
    
    loss_array = []
    bleu_array = []

    for epoch in range(1, epoches + 1):
        for iter in range(1, len(training_pairs) + 1):
            training_pair  = training_pairs[iter - 1]
            input_tensor = training_pair[0]
            target_tensor = training_pair[1]

            #########################################
            # ğŸ” train í•¨ìˆ˜ê°€ ìš°ë¦¬ê°€ ì±„ìš¸ ë¶€ë¶„
            #########################################
            loss = train(input_tensor, target_tensor, encoder, 
                        decoder, encoder_optimizer, decoder_optimizer, criterion, 
                        MAX_LENGTH, device)
            
            loss_total += loss 
            iter_total += 1
          
        elapsed = time.time() - start
        # print("ì…ë ¥ ë¬¸ì¥: {} ê¸°ëŒ€ ì¶œë ¥ ë¬¸ì¥: {}".format(input_tensor.tolist(), target_tensor.tolist()))
        print('epoch : %d\telapsed time: %.2f min\t avg_loss: %.2f' % (epoch, elapsed / 60, loss_total / iter_total))
        bleu_sc = evaluate(encoder, decoder, source_index2word, source_word2index, target_index2word, target_word2index, 
            MAX_LENGTH, device)

        loss_array.append(loss_total / iter_total)
        bleu_array.append(bleu_sc)

    print('<FINISHED TRAINING-------------->')

    import csv    

    with open("output.csv", "w") as f:
        wr = csv.writer(f)
        wr.writerow(["Epoch", "Loss", "BLEU"])    
        for a in range(0, epoches):
            wr.writerow([a + 1, loss_array[a], bleu_array[a]])
    
def test_only():
    #########################################
    # cudaë¥¼ ì‚¬ìš©í•  ê²ƒì¸ì§€, cpuë¥¼ ì‚¬ìš©í•  ê²ƒì¸ì§€ ì„ íƒ
    #########################################
    print("device setting...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print("we will use...."+device+"!\n")
    device = torch.device(device)

    #########################################
    # encoder, decoder ë¶ˆëŸ¬ì˜¤ê¸° (âš ï¸ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆì„ ë•Œë§Œ ì‚¬ìš©í•  ê²ƒ)
    #########################################
    encoder = torch.load('ENCODER')
    decoder = torch.load('DECODER')
    print("finished loading encoder and decoder as a file")  

    #########################################
    # tokenizing and preprocessing of train data
    #########################################
    print("tokenizing...")
    source_texts, target_texts, target_labels = tokenize('data/eng-fra_train.txt')

    source_word2index, source_index2word = preprocess(source_texts)
    target_word2index, target_index2word = preprocess(target_texts)

    source_ind_texts = wordtext2indtext(source_texts, source_word2index)
    target_ind_texts = wordtext2indtext(target_texts, target_word2index)
    target_ind_labels = wordtext2indtext(target_labels, target_word2index)

    source_max_length = max([len(each) for each in source_ind_texts])
    target_max_length = max([len(each) for each in target_ind_texts])
    MAX_LENGTH = max(source_max_length, target_max_length)

    print("------------------------------------")
    print("SOURCE WORD2INDEX MAX LENGTH : ", source_max_length)
    print("TARGET WORD2INDEX MAX LENGTH : ", target_max_length)

    #########################################
    # í…ŒìŠ¤íŠ¸ ì§„í–‰
    #########################################
    with torch.no_grad():
        evaluate(encoder, decoder, source_index2word, source_word2index, target_index2word, target_word2index, 
                MAX_LENGTH, device)

if __name__ == "__main__":
    main()